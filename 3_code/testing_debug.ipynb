{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:22:58.292605Z",
     "start_time": "2025-09-11T13:22:52.679560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from classes import FullFileForTestings  # Import dataset class and model\n",
    "from classes import SpeedEstimatorRNNModified, SpeedEstimatorLSTMModified, SpeedEstimatorGRUModified\n",
    "import os\n",
    "import pandas as pd"
   ],
   "id": "cd89150f1a3960e9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:23:16.752827Z",
     "start_time": "2025-09-11T13:23:16.700328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Loading in the hyperparameters\n",
    "hyperparams_loc = \"../2_trained_models/GRU/trained_models/i7/it_4_norm/hyperparams_GRU_it_4.csv\"\n",
    "hyperparams_df = pd.read_csv(hyperparams_loc, delimiter=\";\")\n",
    "\n",
    "#Reading in the num of models and model locations\n",
    "model_folder_loc = \"../2_trained_models/GRU/trained_models/i7/it_4_norm/state_models/lon/\"\n",
    "pt_files = [f for f in os.listdir(model_folder_loc) if f.endswith(\".pt\")]\n",
    "\n",
    "num_of_models = len(pt_files)\n",
    "\n",
    "column_names = [None] * (num_of_models + 3)\n",
    "for i in range(num_of_models):\n",
    "    if i <= num_of_models:\n",
    "        column_names[i + 1] = pt_files[i]\n",
    "\n",
    "column_names[0] = 'time'\n",
    "column_names[num_of_models + 1] = 'veh_u'\n",
    "column_names[num_of_models + 2] = 'veh_v'\n",
    "\n",
    "#Creating the dataframe for the storing the results\n",
    "results_df = pd.DataFrame(columns = column_names)\n",
    "\n",
    "#Loading in the reference speeds\n",
    "validation_data_loc = \"../1_data/i7/it_1/it_1_100_norm/3_validation/\"\n",
    "validation_files = [f for f in os.listdir(validation_data_loc) if f.endswith(\".csv\")]\n",
    "\n",
    "csv_save_loc = \"../2_trained_models/GRU/trained_models/i7/it_4_norm/results/\""
   ],
   "id": "bb2730a34dc746a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T13:23:17.483745Z",
     "start_time": "2025-09-11T13:23:16.754836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(validation_files)):\n",
    "    validation_df = pd.read_csv(validation_data_loc + validation_files[i], delimiter=\",\")\n",
    "\n",
    "    results_df['time'] = validation_df['Time']\n",
    "    results_df['veh_u'] = validation_df['veh_u']\n",
    "    results_df['veh_v'] = validation_df['veh_v']\n",
    "\n",
    "    #The test dataset is prepared for to be fed in to the model\n",
    "    validation_dataset = FullFileForTestings(validation_data_loc + validation_files[i])\n",
    "    features, actual_speeds = validation_dataset.get_full_data()\n",
    "\n",
    "    for j in range(num_of_models):\n",
    "\n",
    "        #Loading in the hyperparameters\n",
    "        input_size = int(hyperparams_df[\"input_size\"][j])\n",
    "        hidden_size = int(hyperparams_df[\"hidden_size\"][j])\n",
    "        num_layers = int(hyperparams_df[\"num_of_layers\"][j])\n",
    "        output_size = int(hyperparams_df[\"output_size\"][j])\n",
    "\n",
    "        # Pass all data to the model directly\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        features = features.unsqueeze(0)  # Add batch dimension\n",
    "        features = features.to(device)\n",
    "\n",
    "        # Define and load the model\n",
    "        model = SpeedEstimatorLSTMModified(input_size, hidden_size, num_layers, output_size)  # Instantiate your LSTM model\n",
    "        checkpoint = torch.load(model_folder_loc + pt_files[j], map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # Load the trained weights\n",
    "        model = model.to(device)  # Move model to the appropriate device\n",
    "        model.eval()  # Set model to eval model\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            predicted_speeds = predictions.cpu().numpy().flatten()\n",
    "\n",
    "        if output_size == 1:\n",
    "            u = [0] * len(predicted_speeds)\n",
    "            u = predicted_speeds\n",
    "\n",
    "            results_df[pt_files[i]] = u\n",
    "        else:\n",
    "            length = len(predicted_speeds)/2\n",
    "            u = [0]*int(length)\n",
    "            v = [0]*int(length)\n",
    "\n",
    "            for k in range(len(predicted_speeds)):\n",
    "                if k % 2 == 0:\n",
    "                    u[int(k/2)] = predicted_speeds[k]\n",
    "                else:\n",
    "                    v[int((k-1)/2)] = predicted_speeds[k]\n",
    "\n",
    "    results_df.to_csv(csv_save_loc + validation_files[i], index=False)\n",
    "    results_df = results_df.iloc[0:0]"
   ],
   "id": "79d9a48d9322a48c",
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Define and load the model\u001B[39;00m\n\u001B[0;32m     26\u001B[0m model \u001B[38;5;241m=\u001B[39m SpeedEstimatorLSTMModified(input_size, hidden_size, num_layers, output_size)  \u001B[38;5;66;03m# Instantiate your LSTM model\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m checkpoint \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_folder_loc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpt_files\u001B[49m\u001B[43m[\u001B[49m\u001B[43mj\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(checkpoint[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_state_dict\u001B[39m\u001B[38;5;124m'\u001B[39m])  \u001B[38;5;66;03m# Load the trained weights\u001B[39;00m\n\u001B[0;32m     30\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# Move model to the appropriate device\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\serialization.py:1470\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1462\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[0;32m   1463\u001B[0m                     opened_zipfile,\n\u001B[0;32m   1464\u001B[0m                     map_location,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1467\u001B[0m                     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args,\n\u001B[0;32m   1468\u001B[0m                 )\n\u001B[0;32m   1469\u001B[0m             \u001B[38;5;28;01mexcept\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m-> 1470\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mUnpicklingError(_get_wo_message(\u001B[38;5;28mstr\u001B[39m(e))) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1471\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _load(\n\u001B[0;32m   1472\u001B[0m             opened_zipfile,\n\u001B[0;32m   1473\u001B[0m             map_location,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1476\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_load_args,\n\u001B[0;32m   1477\u001B[0m         )\n\u001B[0;32m   1478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mmap:\n",
      "\u001B[1;31mUnpicklingError\u001B[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001B[1mdo those steps only if you trust the source of the checkpoint\u001B[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for j in range(len(validation_files)):\n",
    "\n",
    "    validation_df = pd.read_csv(validation_data_loc + validation_files[j], delimiter=\",\")\n",
    "\n",
    "    results_df['time'] = validation_df['Time']\n",
    "    results_df['veh_u'] = validation_df['veh_u']\n",
    "    results_df['veh_v'] = validation_df['veh_v']\n",
    "\n",
    "    #The test dataset is prepared for to be fed in to the model\n",
    "    validation_dataset = FullFileForTestings(validation_data_loc + validation_files[j])\n",
    "    features, actual_speeds = validation_dataset.get_full_data()\n",
    "\n",
    "    for i in range(num_of_models):\n",
    "\n",
    "        #Loading in the hyperparameters\n",
    "        input_size = int(hyperparams_df[\"input_size\"][i])\n",
    "        hidden_size = int(hyperparams_df[\"hidden_size\"][i])\n",
    "        num_layers = int(hyperparams_df[\"num_of_layers\"][i])\n",
    "        output_size = int(hyperparams_df[\"output_size\"][i])\n",
    "\n",
    "        #Define and load the model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Prepare features for inference\n",
    "        features = features.unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "        model_loc = model_folder_loc + pt_files[i]\n",
    "        checkpoint = torch.load(model_loc, map_location=device, weights_only=False)\n",
    "\n",
    "        model = SpeedEstimatorGRUModified(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "         # Inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            predicted_speeds = predictions.cpu().numpy().flatten()\n",
    "\n",
    "        if output_size == 1:\n",
    "            results_df[pt_files[i]] = predicted_speeds\n",
    "        #Itt le kell majd kezelni azt, amikor két kimenet van\n",
    "\n",
    "    results_df.to_csv(csv_save_loc + validation_files[j], index=False)\n",
    "    results_df = results_df.iloc[0:0]\n"
   ],
   "id": "ac709949f071af73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T11:25:10.879408Z",
     "start_time": "2025-06-06T11:24:59.006087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(num_models):\n",
    "    # Set paths and parameters for testing\n",
    "    model_path = model_location + str(i) + '.pt'  # Replace with the path to your saved model\n",
    "\n",
    "    for j in range(len(num_test_data_path)):\n",
    "        test_csv_path = num_test_data_path[j]\n",
    "        # Prepare the test dataset\n",
    "        test_dataset = FullFileForTestings(test_csv_path)\n",
    "        features, actual_speeds = test_dataset.get_full_data()\n",
    "\n",
    "        # Pass all data to the model directly\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        features = features.unsqueeze(0)  # Add batch dimension\n",
    "        features = features.to(device)\n",
    "\n",
    "        # Define and load the model\n",
    "        model = SpeedEstimatorLSTMModified(input_size, hidden_size[i], num_layers[i], output_size)  # Instantiate your LSTM model\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # Load the trained weights\n",
    "        model = model.to(device)  # Move model to the appropriate device\n",
    "        model.eval()  # Set model to eval model\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            predictions = model(features)\n",
    "            predicted_speeds = predictions.cpu().numpy().flatten()\n",
    "\n",
    "        length = len(predicted_speeds)/2\n",
    "        u = [0]*int(length)\n",
    "        v = [0]*int(length)\n",
    "\n",
    "        for k in range(len(predicted_speeds)):\n",
    "            if k % 2 == 0:\n",
    "                u[int(k/2)] = predicted_speeds[k]\n",
    "            else:\n",
    "                v[int((k-1)/2)] = predicted_speeds[k]\n",
    "\n"
   ],
   "id": "6f9a52a21169108c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "------------------------------------------------------------------------------------------------\n",
    "debugging"
   ],
   "id": "c3a4d4604c0bae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plotting\n",
    "# longitudinal plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actual_speeds[:, 0], label=\"Actual Speed\")\n",
    "plt.plot(u, label=\"Estimated Speed\", linestyle=\"dashed\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Actual vs. Predicted Longitudinal Speed (\" + test_data[j] + \")\")\n",
    "plt.xlabel(\"Time Step [s]\")\n",
    "plt.ylabel(\"Speed [m/s]\")\n",
    "plt.grid()\n",
    "plt.savefig(plot_location + str(i) + \"/u/model_LSTM_\" + str(i) + \"_\" + test_data[j] + \"_act_vs_predicted_u.png\", dpi=300,)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actual_speeds[:, 0].numpy() - u, label=\"Difference\")\n",
    "plt.legend()\n",
    "plt.title(\"Difference of Actual vs. Predicted Longitudinal Speed (\" + test_data[j] + \")\")\n",
    "plt.xlabel(\"Time Step [s]\")\n",
    "plt.ylabel(\"Speed difference [m/s]\")\n",
    "plt.grid()\n",
    "plt.savefig(plot_location + str(i) + \"/u/model_LSTM_\" + str(i) + \"_\" + test_data[j] + \"_act_vs_predicted_u_diff.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# lateral plots\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actual_speeds[:, 1], label=\"Actual Speed\")\n",
    "plt.plot(v, label=\"Estimated Speed\", linestyle=\"dashed\")\n",
    "plt.legend()\n",
    "plt.title(\"Comparison of Actual vs. Predicted Lateral Speed (\" + test_data[j] + \")\")\n",
    "plt.xlabel(\"Time Step [s]\")\n",
    "plt.ylabel(\"Speed [m/s]\")\n",
    "plt.grid()\n",
    "plt.savefig(plot_location + str(i) + \"/v/model_LSTM_\" + str(i) + \"_\" + test_data[j] + \"_act_vs_predicted_v.png\", dpi=300,)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actual_speeds[:, 1].numpy() - v, label=\"Difference\")\n",
    "plt.legend()\n",
    "plt.title(\"Difference of Actual vs. Predicted Lateral Speed (\" + test_data[j] + \")\")\n",
    "plt.xlabel(\"Time Step [s]\")\n",
    "plt.ylabel(\"Speed difference [m/s]\")\n",
    "plt.grid()\n",
    "plt.savefig(plot_location + str(i) + \"/v/model_LSTM_\" + str(i) + \"_\" + test_data[j] + \"_act_vs_predicted_v_diff.png\", dpi=300)\n",
    "plt.close()"
   ],
   "id": "7e3ca3d825490a0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
